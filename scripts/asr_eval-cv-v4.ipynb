{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES AND DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2ForCTC, WavLMForCTC, WhisperForConditionalGeneration, Wav2Vec2Processor, WhisperProcessor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import jiwer\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore specific warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\".*transcription using a multilingual Whisper will default to language detection.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Passing a tuple of `past_key_values` is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The attention mask is not set and cannot be inferred from input.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and processor parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"wavlm\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    },\n",
    "    \"whisper\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    },\n",
    "    \"wav2vec2\": {\n",
    "        \"model\": None,\n",
    "        \"processor\": None,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")  # Print the device being used\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")  # Divider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models and processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    try:\n",
    "        print(\"Loading models and processors...\\n\")\n",
    "        for model_name in models.keys():\n",
    "            print(f\"Loading {model_name}...\\n\")\n",
    "            try:\n",
    "                if model_name == \"wav2vec2\":\n",
    "                    models[model_name][\"model\"] = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
    "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "                elif model_name == \"wavlm\":\n",
    "                    models[model_name][\"model\"] = WavLMForCTC.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\").to(device)\n",
    "                    models[model_name][\"processor\"] = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-large\")\n",
    "                elif model_name == \"whisper\":\n",
    "                    models[model_name][\"model\"] = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to(device)\n",
    "                    models[model_name][\"processor\"] = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "                print(f\"\\n{model_name} loaded successfully.\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name}: {e}\\n\")\n",
    "        print(\"All models and processors loaded.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Common Voice dataset (TSV and mp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_common_voice_data(tsv_file, audio_dir, max_samples=100):\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    count = 0  # Initialize a counter for processed files\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # Step 1: Load TSV file\n",
    "        df = pd.read_csv(tsv_file, sep='\\t')\n",
    "\n",
    "        # Shuffle the DataFrame to get random samples\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Step 2: Process each row in the TSV file\n",
    "        for index, row in df.iterrows():\n",
    "            audio_file = row['path']\n",
    "            if not audio_file.endswith(\".mp3\"):\n",
    "                audio_file += \".mp3\"\n",
    "\n",
    "            transcript = row['sentence']  # Extract transcript from the 'sentence' column\n",
    "\n",
    "            # Append to the list of files and transcripts\n",
    "            audio_files.append(os.path.join(audio_dir, audio_file))\n",
    "            transcripts.append(transcript)\n",
    "            count += 1\n",
    "\n",
    "            # Stop once max_samples have been loaded\n",
    "            if count >= max_samples:\n",
    "                print(f\"Finished loading {count} audio files and transcripts from dataset.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "                break\n",
    "\n",
    "        return audio_files, transcripts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Common Voice data: {e}\\n\")\n",
    "        return [], []  # Return empty lists on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(reference, hypothesis):\n",
    "    # Normalize to lowercase\n",
    "    reference = reference.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "    \n",
    "    # Tokenize the transcriptions\n",
    "    reference_words = reference.split()\n",
    "    hypothesis_words = hypothesis.split()\n",
    "\n",
    "    # Calculate WER and CER\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    cer = jiwer.cer(reference, hypothesis)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    true_positives = sum(1 for word in hypothesis_words if word in reference_words)\n",
    "    false_positives = len(hypothesis_words) - true_positives\n",
    "    false_negatives = len(reference_words) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate accuracy based on the number of correct words\n",
    "    correct_predictions = sum(1 for i in range(min(len(reference_words), len(hypothesis_words))) if reference_words[i] == hypothesis_words[i])\n",
    "    accuracy = correct_predictions / len(reference_words) if reference_words else 0\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad or truncate audio input to a target length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(array, target_length):\n",
    "    current_length = array.shape[1]\n",
    "    if current_length > target_length:\n",
    "        return array[:, :target_length]\n",
    "    elif current_length < target_length:\n",
    "        pad_width = ((0, 0), (0, target_length - current_length))\n",
    "        return np.pad(array, pad_width, mode='constant')\n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to evaluate a model on Common Voice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, audio_files, transcripts):\n",
    "    results = {}\n",
    "    total_loss = 0\n",
    "    last_transcription = \"\"  # Store the last transcription\n",
    "    last_audio_file = \"\"  # Store the last audio file\n",
    "\n",
    "    # Initialize cumulative sums for metrics\n",
    "    total_metrics = {\n",
    "        \"wer\": 0,\n",
    "        \"cer\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"f1_score\": 0,\n",
    "        \"accuracy\": 0\n",
    "    }\n",
    "    num_samples = len(audio_files)\n",
    "\n",
    "    try:\n",
    "        print(f\"Evaluating {model.__class__.__name__}...\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        for i, audio_file in enumerate(audio_files):\n",
    "            try:\n",
    "                # Load mp3 audio file\n",
    "                audio, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "                # Resample if necessary\n",
    "                if sample_rate != 16000:\n",
    "                    resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                    audio = resampler(audio)\n",
    "\n",
    "                # Preprocess audio and move to device\n",
    "                if isinstance(model, WhisperForConditionalGeneration):\n",
    "                    input_features = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "                else:\n",
    "                    inputs = processor(audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "                    input_values = inputs.input_values.to(device)\n",
    "\n",
    "                    # Ensure consistent input size\n",
    "                    target_length = 200000  # Adjust as needed\n",
    "                    input_values = torch.tensor(pad_or_truncate(input_values.cpu().numpy(), target_length)).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(model, WhisperForConditionalGeneration):\n",
    "                        output = model.generate(input_features, language='en')\n",
    "                        transcription = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "                    else:\n",
    "                        output = model(input_values)\n",
    "                        logits = output.logits\n",
    "                        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "                    loss = 0  # Loss not computed during inference\n",
    "                    total_loss += loss\n",
    "\n",
    "                # Calculate metrics\n",
    "                metrics = calculate_metrics(transcripts[i], transcription)\n",
    "\n",
    "                # Accumulate metrics\n",
    "                for key in total_metrics:\n",
    "                    total_metrics[key] += metrics[key]\n",
    "\n",
    "                # Store the last audio file and its transcription\n",
    "                last_audio_file = audio_file\n",
    "                last_transcription = transcription\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating file {audio_file}: {e}\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        \n",
    "        print(f\"Finished evaluating {model.__class__.__name__}.\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        if num_samples > 0:\n",
    "            avg_metrics = {key: value / num_samples for key, value in total_metrics.items()}\n",
    "            results = avg_metrics\n",
    "        else:\n",
    "            results = {key: 0 for key in total_metrics}  # Default to zeros if no samples were processed\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model: {e}\\n\")\n",
    "\n",
    "    results['last_transcription'] = last_transcription\n",
    "    results['last_audio_file'] = last_audio_file\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function to run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    load_models()  # Load models and processors\n",
    "    audio_dir = \"data/common-voice/clips\"  # Set your audio directory path\n",
    "    tsv_file = \"data/common-voice/validated.tsv\"  # Set your TSV file path\n",
    "    audio_files, transcripts = load_common_voice_data(tsv_file, audio_dir, max_samples=100)\n",
    "\n",
    "    # Evaluate each model\n",
    "    results = {}\n",
    "    for model_name in models.keys():\n",
    "        results[model_name] = evaluate_model(models[model_name][\"model\"], models[model_name][\"processor\"], audio_files, transcripts)\n",
    "\n",
    "    # Print results in the desired format\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        if metrics:  # Check if metrics is not empty\n",
    "            print(f\"WER: {metrics.get('wer', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"CER: {metrics.get('cer', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"Accuracy: {metrics.get('accuracy', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"Precision: {metrics.get('precision', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"Recall: {metrics.get('recall', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"F1 Score: {metrics.get('f1_score', 0) * 100:.2f}%\")  # Convert to percentage\n",
    "            print(f\"Last Transcription: '{metrics.get('last_transcription', 'N/A')}'\")  # Print last transcription\n",
    "            print(f\"Last Audio File: '{metrics.get('last_audio_file', 'N/A')}'\")  # Print last audio file\n",
    "            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "        else:\n",
    "            print(\"Metrics could not be calculated.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
